{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debiasing Imputation #\n",
    "\n",
    "This notebook is about dealing with missing data that does not increase bias (gender bias, race bias, etc.), or even potentially reduce it\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem statement ##\n",
    "Most common way to handle missing data is to drop them. The second most common way is to replace the missing data with the most likely value. For the categorical features it is the most frequent value. For the numerical features it is the mean. `scikit-learn` has a class available for this: [SimpleImputer](http://scikit-learn.org/dev/modules/generated/sklearn.impute.SimpleImputer.html). The problem with this approach is that even though it preserves mean, but it reduces the standard deviation, sometimes very significantly. To demonstrate this, let's consider a simple array, then remove half of the values and replace them with mean, and see what happens with STD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.53547966, 0.99260019, 0.88633099, 1.21320929, 1.03287069,\n",
       "       1.34151072, 0.98476757, 1.17019719, 1.10089714, 0.48023982,\n",
       "       1.49781353, 1.21862054, 1.91732282, 0.55931941, 0.53091708,\n",
       "       1.3266663 , 0.94301855, 1.1107632 , 0.42426201, 1.39311814])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import norm, multinomial\n",
    "original_data = norm.rvs(loc=1.0, scale=0.5, size=1000, random_state=1386)\n",
    "original_data[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.53547966, 1.        , 0.88633099, 1.        , 1.03287069,\n",
       "       1.        , 0.98476757, 1.        , 1.10089714, 1.        ,\n",
       "       1.49781353, 1.        , 1.91732282, 1.        , 0.53091708,\n",
       "       1.        , 0.94301855, 1.        , 0.42426201, 1.        ])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Now replace every other element with the mean 1.0\n",
    "missing_elements = np.asarray([0,1]*500)\n",
    "updated_data = original_data * (1-missing_elements) + missing_elements\n",
    "updated_data[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 1.0117580053066189, std: 0.33428315977079176\n"
     ]
    }
   ],
   "source": [
    "#Now, let's get mean and std of the new distribution:\n",
    "mean, std = norm.fit(updated_data)\n",
    "print(f'Mean: {mean}, std: {std}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you see, even though the mean is the same, the standard deviation is much less. While the imputation of data this way increases the performance of the model, it also amplifies the bias that already exists in the data. In order to prevent amplification of the bias, we have to replace the missing values with a sample from the normal distribution with the same mean and standard deviation. For categorical features it would be a multinomial distribution.\n",
    "\n",
    "For debiasing we can try to increase the standard deviation of the distribution from which we sample data for numerical features, and a similar transformation for the multinomial distribution. \n",
    "\n",
    "In this notebook I suggest two classes for the numerical and categorical features respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proposed solution ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import numpy.ma as ma\n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "class NumericalUnbiasingImputer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Un-biasing imputation transformer for completing missing values.\n",
    "        Parameters\n",
    "        ----------\n",
    "        std_scaling_factor : number\n",
    "            We will multiply std by this factor to increase or decrease bias\n",
    "    \"\"\"\n",
    "    def __init__(self, std_scaling_factor=1, random_state=7294):\n",
    "        self.std_scaling_factor = std_scaling_factor\n",
    "        self.random_state = random_state\n",
    "\n",
    "        \n",
    "    def fit(self, X: np.ndarray, y=None):\n",
    "        \"\"\"Fit the imputer on X.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
    "            Input data, where ``n_samples`` is the number of samples and\n",
    "            ``n_features`` is the number of features.\n",
    "        Returns\n",
    "        -------\n",
    "        self : NumericalUnbiasingImputer\n",
    "        \"\"\"\n",
    "        if len(X.shape) < 2:\n",
    "            X = X.reshape(-1,1)\n",
    "        mask = np.isnan(X)\n",
    "        masked_X = ma.masked_array(X, mask=mask)\n",
    "\n",
    "        mean_masked = np.ma.mean(masked_X, axis=0)\n",
    "        std_masked = np.ma.std(masked_X, axis=0)\n",
    "        mean = np.ma.getdata(mean_masked)\n",
    "        std = np.ma.getdata(std_masked)\n",
    "        mean[np.ma.getmask(mean_masked)] = np.nan\n",
    "        std[np.ma.getmask(std_masked)] = np.nan\n",
    "        self.mean_ = mean\n",
    "        self.std_ = std * self.std_scaling_factor\n",
    "\n",
    "        return self\n",
    "    \n",
    "     \n",
    "    def transform(self, X):\n",
    "        \"\"\"Impute all missing values in X.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like}, shape (n_samples, n_features)\n",
    "            The input data to complete.\n",
    "        \"\"\"\n",
    "        check_is_fitted(self, ['mean_', 'std_'])\n",
    "\n",
    "        if len(X.shape) < 2:\n",
    "            X = X.reshape(-1,1)\n",
    "        mask = np.isnan(X)\n",
    "        n_missing = np.sum(mask, axis=0)\n",
    "        \n",
    "        def transform_single(index):\n",
    "            col = X[:,index].copy()\n",
    "            mask_col = mask[:, index]\n",
    "            sample = np.asarray(norm.rvs(loc=self.mean_[index], scale=self.std_[index], \n",
    "                                         size=col.shape[0], random_state=self.random_state))\n",
    "            col[mask_col] = sample[mask_col]\n",
    "            return col\n",
    "            \n",
    "        \n",
    "        Xnew = np.vstack([transform_single(index) for index,_ in enumerate(n_missing)]).T\n",
    "        \n",
    "\n",
    "        return Xnew\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.53547966  7.6773983 ]\n",
      " [ 1.28446105  4.96300096]\n",
      " [ 0.88633099  4.43165496]\n",
      " [ 1.26161414  6.06604643]\n",
      " [ 1.03287069  5.16435346]\n",
      " [ 1.54999452  6.70755359]\n",
      " [ 0.98476757  4.92383783]\n",
      " [ 1.78611804  5.85098593]\n",
      " [ 1.10089714  5.50448571]\n",
      " [ 1.01113317  2.4011991 ]\n",
      " [ 1.49781353  7.48906767]\n",
      " [ 1.2508774   6.09310269]\n",
      " [ 1.91732282  9.58661411]\n",
      " [ 1.13145139  2.79659703]\n",
      " [ 0.53091708  2.65458541]\n",
      " [ 1.81200067  6.6333315 ]\n",
      " [ 0.94301855  4.71509276]\n",
      " [-0.0711673   5.55381598]\n",
      " [ 0.42426201  2.12131004]\n",
      " [ 1.94678859  6.96559068]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1000, 2)"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imputer = NumericalUnbiasingImputer()\n",
    "missing_indicator = missing_elements.copy().astype(np.float16)\n",
    "missing_indicator[missing_indicator == 1] = np.nan\n",
    "data_with_missing_values = original_data + missing_indicator\n",
    "data_with_missing_values = np.vstack([data_with_missing_values, original_data*5]).T\n",
    "imputer.fit(data_with_missing_values)\n",
    "transformed = imputer.transform(data_with_missing_values)\n",
    "print(transformed[:20,:])\n",
    "transformed.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 1.0197348250784546, Std: 0.4659586665233841\n"
     ]
    }
   ],
   "source": [
    "#Let's see how it is different from the original array:\n",
    "new_mean, new_std = norm.fit(transformed[:,0])\n",
    "print(f'Mean: {new_mean}, Std: {new_std}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some difference in the standard deviation can be explained, because we fitted the model on the incomplete data.\n",
    "\n",
    "Now we need to do the same for the categorical features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the multinomial distribution there is no single parameter responsible for standard deviation. However we can observe, that scaling the standard deviation of the normal distribution is equivalent to scaling `x`. If we do a similar transformation in the multinomial distribution, this would be equivalent to raising the parameters to the power of $\\frac{1}{s}$, where $s$ is the scaling factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "class CategoricalUnbiasingImputer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Un-biasing imputation transformer for completing missing values.\n",
    "        Parameters\n",
    "        ----------\n",
    "        std_scaling_factor : number\n",
    "            We will multiply std by this factor to increase or decrease bias\n",
    "    \"\"\"\n",
    "    def __init__(self, scaling_factor=1, random_state=7294):\n",
    "        self.scaling_factor = scaling_factor\n",
    "        self.random_state = random_state\n",
    "\n",
    "        \n",
    "    def fit(self, X: np.ndarray, y=None):\n",
    "        \"\"\"Fit the imputer on X.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
    "            Input data, where ``n_samples`` is the number of samples and\n",
    "            ``n_features`` is the number of features.\n",
    "        Returns\n",
    "        -------\n",
    "        self : NumericalUnbiasingImputer\n",
    "        \"\"\"\n",
    "        if len(X.shape) < 2:\n",
    "            X = X.reshape(-1,1)\n",
    "\n",
    "        def fit_column(column):\n",
    "            mask = pd.isnull(column)\n",
    "            column = column[~mask]\n",
    "            unique_values, counts = np.unique(column.data, return_counts=True)\n",
    "            total = sum(counts)\n",
    "            probabilities = np.array([(count/total)**(1/self.scaling_factor) \n",
    "                    for count in counts])\n",
    "            total_probability = sum(probabilities)\n",
    "            probabilities /= total_probability\n",
    "            return unique_values, probabilities\n",
    "\n",
    "\n",
    "        self.statistics_ = [fit_column(X[:,column]) for column in range(X.shape[1])]\n",
    "\n",
    "        return self\n",
    "    \n",
    "     \n",
    "    def transform(self, X):\n",
    "        \"\"\"Impute all missing values in X.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like}, shape (n_samples, n_features)\n",
    "            The input data to complete.\n",
    "        \"\"\"\n",
    "        check_is_fitted(self, ['statistics_'])\n",
    "\n",
    "        if len(X.shape) < 2:\n",
    "            X = X.reshape(-1,1)\n",
    "        \n",
    "        def transform_single(index):\n",
    "            column = X[:,index].copy()\n",
    "            mask = pd.isnull(column)\n",
    "            values, probabilities = self.statistics_[index]\n",
    "\n",
    "            sample = np.argmax(multinomial.rvs(p=probabilities, n=1,\n",
    "                                         size=mask.sum(), random_state=self.random_state), axis=1)\n",
    "            column[mask] = np.vectorize(lambda pick: values[pick])(sample);\n",
    "            return column\n",
    "            \n",
    "        \n",
    "        Xnew = np.vstack([transform_single(index) for index in range(len(self.statistics_))]).T\n",
    "        \n",
    "\n",
    "        return Xnew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(array(['four', 'one', 'three', 'two'], dtype=object), array([0.14285714, 0.42857143, 0.14285714, 0.28571429]))]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([['one'],\n",
       "       ['one'],\n",
       "       ['two'],\n",
       "       ['three'],\n",
       "       ['four'],\n",
       "       ['one'],\n",
       "       ['one'],\n",
       "       ['one'],\n",
       "       ['two']], dtype=object)"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names = np.array(['one', None, 'two', 'three', 'four', 'one', None, 'one', 'two'])\n",
    "names = names.reshape(-1,1)\n",
    "cat_imp = CategoricalUnbiasingImputer(random_state=121)\n",
    "cat_imp.fit(names)\n",
    "print(cat_imp.statistics_)\n",
    "imputed = cat_imp.transform(names)\n",
    "imputed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we test our utilities on the scikit-learn datasets. Let's try our approach on the famous titanic dataset. Let's see if any of the columns contain nans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PassengerId      0\n",
       "Survived         0\n",
       "Pclass           0\n",
       "Name             0\n",
       "Sex              0\n",
       "Age            177\n",
       "SibSp            0\n",
       "Parch            0\n",
       "Ticket           0\n",
       "Fare             0\n",
       "Cabin          687\n",
       "Embarked         2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic = pd.read_csv(\"data/titanic.csv\")\n",
    "titanic.isna().sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 12 columns):\n",
      "PassengerId    891 non-null int64\n",
      "Survived       891 non-null int64\n",
      "Pclass         891 non-null int64\n",
      "Name           891 non-null object\n",
      "Sex            891 non-null object\n",
      "Age            714 non-null float64\n",
      "SibSp          891 non-null int64\n",
      "Parch          891 non-null int64\n",
      "Ticket         891 non-null object\n",
      "Fare           891 non-null float64\n",
      "Cabin          204 non-null object\n",
      "Embarked       889 non-null object\n",
      "dtypes: float64(2), int64(5), object(5)\n",
      "memory usage: 83.6+ KB\n"
     ]
    }
   ],
   "source": [
    "titanic.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that Age is numeric, and Cabin is an object feature. Let's update them using our imputers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\michael\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:28: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "C:\\Users\\michael\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:54: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n"
     ]
    }
   ],
   "source": [
    "n_imputer = NumericalUnbiasingImputer()\n",
    "titanic.Age = n_imputer.fit(titanic.Age).transform(titanic.Age)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\michael\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:26: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "C:\\Users\\michael\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:55: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n"
     ]
    }
   ],
   "source": [
    "c_imputer = CategoricalUnbiasingImputer()\n",
    "titanic.Cabin = c_imputer.fit(titanic.Cabin).transform(titanic.Cabin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     22.000000\n",
       "1     38.000000\n",
       "2     26.000000\n",
       "3     35.000000\n",
       "4     35.000000\n",
       "5     45.875319\n",
       "6     54.000000\n",
       "7      2.000000\n",
       "8     27.000000\n",
       "9     14.000000\n",
       "10     4.000000\n",
       "11    58.000000\n",
       "12    20.000000\n",
       "13    39.000000\n",
       "14    14.000000\n",
       "15    55.000000\n",
       "16     2.000000\n",
       "17    -3.935337\n",
       "18    31.000000\n",
       "19    58.066929\n",
       "Name: Age, dtype: float64"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Let's see how it transformed Age\n",
    "titanic.Age.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, negative age is a bit too much... But keep in mind that the purpose is not to reconstruct the data, but to avoid amplifying bias in the machine learning model. Let's make sure Age and Cabin now is not null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(titanic.Age.isnull().sum())\n",
    "print(titanic.Cabin.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['C52', 'C85', 'E36', 'C123', 'D17', 'B80', 'E46', 'C30', 'C87',\n",
       "       'G6', 'C103', 'D6', 'D', 'B51 B53 B55', 'E12', 'D9', 'D33', 'B49',\n",
       "       'A26', 'C126', 'D56', 'C111', 'A6', 'C70', 'D10 D12', 'C78',\n",
       "       'C23 C25 C27', 'F G73', 'F2', 'B50', 'B78', 'B20', 'C47', 'D20',\n",
       "       'C86', 'C93', 'C22 C26', 'C110', 'E38', 'F38', 'B102', 'B35',\n",
       "       'E121', 'C68', 'D36', 'E8', 'A36', 'B30', 'B94', 'B3', 'C128',\n",
       "       'B28', 'C83', 'C2', 'F33', 'B96 B98', 'E31', 'C50', 'D26', 'C125',\n",
       "       'F G63', 'E101', 'D28', 'E50', 'A16', 'F4', 'D35', 'C65', 'A5',\n",
       "       'A7', 'C124', 'C54', 'B58 B60', 'B77', 'E58', 'E63', 'F E69', 'T',\n",
       "       'D47', 'A24', 'B86', 'B37', 'E44', 'A31', 'E25', 'B71', 'B18',\n",
       "       'B101', 'C46', 'E34', 'D49', 'E33', 'C49', 'D45', 'B19', 'C95',\n",
       "       'B22', 'B5', 'A32', 'C7', 'B4', 'D46', 'D11', 'A23', 'E10', 'E24',\n",
       "       'E17', 'D15', 'C90', 'C118', 'B69', 'A20', 'B41', 'E67', 'C99',\n",
       "       'D7', 'E77', 'B42', 'C45', 'C106', 'E49', 'A19', 'C91', 'D19',\n",
       "       'C92', 'B57 B59 B63 B66', 'E40', 'B73', 'C32', 'D50', 'B79', 'D21',\n",
       "       'D37', 'C62 C64', 'C82', 'E68', 'C148', 'A14', 'C104', 'B82 B84',\n",
       "       'A10', 'A34', 'D48', 'B38', 'B39', 'C101', 'D30'], dtype=object)"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Unique values of the Cabin\n",
    "titanic.Cabin.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next Steps #\n",
    "\n",
    "Now we need to find a dataset that is known to have a gender or race bias, and demonstrate, that with this technique we can avoid amplifying bias, and maybe even decrease the bias by applying a scaling factor"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
