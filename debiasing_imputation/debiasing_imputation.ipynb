{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unbiasing Imputer #\n",
    "\n",
    "This notebook is about dealing with missing data that does not increase bias (gender bias, race, bias, etc.), or even potentially reduce it\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem statement ##\n",
    "Most common way to handle missing data is to drop them. The second most common way is to replace the missing data with the most likely value. For the categorical features it is the most frequent value. For the numerical features it is the mean. `scikit-learn` has a class available for this: [SimpleImputer](http://scikit-learn.org/dev/modules/generated/sklearn.impute.SimpleImputer.html). The problem with this approach is that even though it preserves mean, but it reduces the standard deviation, sometimes very significantly. To demonstrate this, let's consider a simple array, then remove half of the values and replace them with mean, and see what happens with STD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.53547966, 0.99260019, 0.88633099, 1.21320929, 1.03287069,\n",
       "       1.34151072, 0.98476757, 1.17019719, 1.10089714, 0.48023982,\n",
       "       1.49781353, 1.21862054, 1.91732282, 0.55931941, 0.53091708,\n",
       "       1.3266663 , 0.94301855, 1.1107632 , 0.42426201, 1.39311814])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "original_data = norm.rvs(loc=1.0, scale=0.5, size=1000, random_state=1386)\n",
    "original_data[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.53547966, 1.        , 0.88633099, 1.        , 1.03287069,\n",
       "       1.        , 0.98476757, 1.        , 1.10089714, 1.        ,\n",
       "       1.49781353, 1.        , 1.91732282, 1.        , 0.53091708,\n",
       "       1.        , 0.94301855, 1.        , 0.42426201, 1.        ])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Now replace every other element with the mean 1.0\n",
    "missing_elements = np.asarray([0,1]*500)\n",
    "updated_data = original_data * (1-missing_elements) + missing_elements\n",
    "updated_data[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 1.0117580053066189, std: 0.33428315977079176\n"
     ]
    }
   ],
   "source": [
    "#Now, let's get mean and std of the new distribution:\n",
    "mean, std = norm.fit(updated_data)\n",
    "print(f'Mean: {mean}, std: {std}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you see, even though the mean is the same, the standard deviation is much less. While the imputation of data this way increases the performance of the model, it also amplifies the bias that already exists in the data. In order to prevent amplification of the bias, we have to replace the missing values with a sample from the normal distribution with the same mean and standard deviation. For categorical features it would be a multinomial distribution.\n",
    "\n",
    "For debiasing we can try to increase the standard deviation of the distribution from which we sample data for numerical features, and a similar transformation for the multinomial distribution. \n",
    "\n",
    "In this notebook I suggest two classes for the numerical and categorical features respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import numpy.ma as ma\n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "class NumericalUnbiasingImputer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Un-biasing imputation transformer for completing missing values.\n",
    "        Parameters\n",
    "        ----------\n",
    "        std_scaling_factor : number\n",
    "            We will multiply std by this factor to increase or decrease bias\n",
    "    \"\"\"\n",
    "    def __init__(self, std_scaling_factor=1, random_state=7294):\n",
    "        self.std_scaling_factor = std_scaling_factor\n",
    "        self.random_state = random_state\n",
    "\n",
    "        \n",
    "    def fit(self, X: np.ndarray, y=None):\n",
    "        \"\"\"Fit the imputer on X.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
    "            Input data, where ``n_samples`` is the number of samples and\n",
    "            ``n_features`` is the number of features.\n",
    "        Returns\n",
    "        -------\n",
    "        self : NumericalUnbiasingImputer\n",
    "        \"\"\"\n",
    "        mask = np.isnan(X)\n",
    "        masked_X = ma.masked_array(X, mask=mask)\n",
    "\n",
    "        mean_masked = np.ma.mean(masked_X, axis=0)\n",
    "        std_masked = np.ma.std(masked_X, axis=0)\n",
    "        mean = np.ma.getdata(mean_masked)\n",
    "        std = np.ma.getdata(std_masked)\n",
    "        mean[np.ma.getmask(mean_masked)] = np.nan\n",
    "        std[np.ma.getmask(std_masked)] = np.nan\n",
    "        self.mean_ = mean\n",
    "        self.std_ = std * self.std_scaling_factor\n",
    "\n",
    "        return self\n",
    "    \n",
    "     \n",
    "    def transform(self, X):\n",
    "        \"\"\"Impute all missing values in X.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like}, shape (n_samples, n_features)\n",
    "            The input data to complete.\n",
    "        \"\"\"\n",
    "        check_is_fitted(self, ['mean_', 'std_'])\n",
    "\n",
    "        mask = np.isnan(X)\n",
    "        n_missing = np.sum(mask, axis=0)\n",
    "        \n",
    "        def transform_single(index):\n",
    "            col = X[:,index].copy()\n",
    "            mask_col = mask[:, index]\n",
    "            sample = np.asarray(norm.rvs(loc=self.mean_[index], scale=self.std_[index], \n",
    "                                         size=col.shape[0], random_state=self.random_state))\n",
    "            col[mask_col] = sample[mask_col]\n",
    "            return col\n",
    "            \n",
    "        \n",
    "        Xnew = np.vstack([transform_single(index) for index,_ in enumerate(n_missing)]).T\n",
    "        \n",
    "\n",
    "        return Xnew\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.53547966  7.6773983 ]\n",
      " [ 1.28446105  4.96300096]\n",
      " [ 0.88633099  4.43165496]\n",
      " [ 1.26161414  6.06604643]\n",
      " [ 1.03287069  5.16435346]\n",
      " [ 1.54999452  6.70755359]\n",
      " [ 0.98476757  4.92383783]\n",
      " [ 1.78611804  5.85098593]\n",
      " [ 1.10089714  5.50448571]\n",
      " [ 1.01113317  2.4011991 ]\n",
      " [ 1.49781353  7.48906767]\n",
      " [ 1.2508774   6.09310269]\n",
      " [ 1.91732282  9.58661411]\n",
      " [ 1.13145139  2.79659703]\n",
      " [ 0.53091708  2.65458541]\n",
      " [ 1.81200067  6.6333315 ]\n",
      " [ 0.94301855  4.71509276]\n",
      " [-0.0711673   5.55381598]\n",
      " [ 0.42426201  2.12131004]\n",
      " [ 1.94678859  6.96559068]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1000, 2)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imputer = NumericalUnbiasingImputer()\n",
    "missing_indicator = missing_elements.copy().astype(np.float16)\n",
    "missing_indicator[missing_indicator == 1] = np.nan\n",
    "data_with_missing_values = original_data + missing_indicator\n",
    "data_with_missing_values = np.vstack([data_with_missing_values, original_data*5]).T\n",
    "imputer.fit(data_with_missing_values)\n",
    "transformed = imputer.transform(data_with_missing_values)\n",
    "print(transformed[:20,:])\n",
    "transformed.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 1.0197348250784546, Std: 0.4659586665233841\n"
     ]
    }
   ],
   "source": [
    "#Let's see how it is different from the original array:\n",
    "new_mean, new_std = norm.fit(transformed[:,0])\n",
    "print(f'Mean: {new_mean}, Std: {new_std}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some difference in the standard deviation can be explained, because we fitted the model on the incomplete data.\n",
    "\n",
    "Now we need to do the same for the categorical features"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
