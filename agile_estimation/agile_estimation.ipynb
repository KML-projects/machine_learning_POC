{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using statistics in agile estimation #\n",
    "\n",
    "In Scrum, and in general in any Agile process, starting with Extreme Programming, the development is split into fixed-length iterations or sprints. Before the beginning of each sprint, the team is doing sprints planning and allocates the work for the sprint based on the priority of user stories. The question is, how many stories the team can complete in one iteration? \n",
    "\n",
    "The unit of work started from \"ideal hours\" in Extreme Programming, which is different from the real hours. Ideal hours is the number of hours available to the team to actually do work. Moreover, the estimate was multiplied by the load factor, initially set to 3. \n",
    "\n",
    "This fairly complex models seemed ineffective and was replaced to story points. They are abstract measure of complexity of the user story, and the values are assigned based on the similar user stories, done in the past, and a ceremony called \"planning poker\". The number of story points the team can do within one iteration is called project velocity. It is measured based on the \"Yesterday's weather\" rule. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of story points completed varies in the past iterations, so it is natural to use statistical methods to model the project velocity. The probability distribution that addresses this problem is [Poisson distribution](https://en.wikipedia.org/wiki/Poisson_distribution):\n",
    "\n",
    "$$ p(k) = \\frac {\\lambda^k e^{-\\lambda}}{k!} $$\n",
    "\n",
    "where p(k) is the probability of completing k user stories, and $\\lambda$ is the project velocity. We can infer the project velocity relatively easily in Python. Let's say after 5 iterations we have these numbers of story points completed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import poisson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=np.array([14, 12,  7, 14, 13])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To infer the parameter of the distribution, we just take an average:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "velocity = data.mean()\n",
    "velocity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, our velocity is 12 story points per iteration. The median of the distribution is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poisson(velocity).median()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means, that there is only 50% chance we can complete all 12 story points in an iteration. Is this what the management is asking for? Probably not. They want a commitment, or at least, 95% confidence. We can also easily get that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poisson(velocity).ppf(0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means, that we can report to the management, that with 95% confidence we can complete 7 story points within one iteration.\n",
    "\n",
    "The project velocity may have a systematic error if the individual developers' velocity is drastically different. A good Agile project not only tracks the team velocity, but also developer velocity, and adjust the former based on the latter.\n",
    "\n",
    "The project velocity may also depend on other factors. The developers are getting better in estimating the stories, the team is becoming faster, so it may not makes sense to use all historical data equally. We can perhaps use exponential moving average:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exponential_moving_average(data, size, weight):\n",
    "    considered = data[data.shape[0] - size:]\n",
    "    exponents = np.arange(size, 0, -1)\n",
    "    weights = weight ** exponents\n",
    "    weights /= sum(weights)\n",
    "    return (considered*weights).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.428571428571427"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "velocity = exponential_moving_average(data, 3, 0.5)\n",
    "velocity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see, that based on the last three iterations our project velocity is slightly higher, than the total velocity. \n",
    "\n",
    "In addition to moving average, we can try to fit data to [Poisson Regression Model](https://en.wikipedia.org/wiki/Poisson_regression), however, in that model the distribution parameter exponentially depends on the linear combination of parameters, so it is unstable for the long range.\n",
    "\n",
    "$$ \\lambda = e^{\\theta x} $$\n",
    "\n",
    "What happens in reality, that the velocity after fluctuating in the earlier stages of the projects tends to settle to a specific value in the later iterations, which, obviously, cannot be modeled with the exponential function. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
