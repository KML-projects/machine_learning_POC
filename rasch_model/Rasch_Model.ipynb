{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Rasch Model #\n",
    "\n",
    "This notebook demonstrates implementation of the Rasch model in TensorFlow. All theoretical parts are taken from an excellent textbook \"Bayesian Reasoning and Machine Learning\" by David Barber. The Rasch Model is covered in chapter 22 of the book."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider an exam in which student $s$ answers question $q$ either correctly $x_{qs} = 1$ or incorrectly $x_{qs} = 0$.\n",
    "For a set of $N$ students and $Q$ questions, the performance of all students is given in the $Q \\times N$ binary\n",
    "matrix $X$. Based on this data alone we wish to evaluate the ability of each student, and at the same time estimate difficulty of each question. To learn both, we assign the probability that a student $s$ gets a question $q$ correct based on the student's latent ability $\\alpha_s$ and the latent difficulty of the question $\\delta_q$:\n",
    "\n",
    "$$p(x_{qs} = 1|\\alpha, \\delta) = \\sigma(\\alpha_s -\\delta_q)$$\n",
    "Where $\\sigma$ is sigmoid function.\n",
    "\n",
    "Making the i.i.d. assumption, the likelihood of the data $X$ under this model is:\n",
    "\n",
    "$$p(X|\\alpha, \\delta) = \\prod_{s=1}^S\\prod_{q=1}^Q \\sigma(\\alpha_s-\\delta_q)^{x_{qs}} (1-\\sigma(\\alpha_s-\\delta_q))^{1-x_{qs}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The log likelihood is then:\n",
    "\n",
    "$$L \\equiv log(X|\\alpha, \\beta) = \\sum_{q,s} { x_{qs} log \\sigma(\\alpha_s - \\delta_q) + \n",
    "(1 - x_{qs}) log (1 - \\sigma(\\alpha_s - \\delta_q))}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the partial derivatives are:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial \\alpha_s} = \\sum_{q=1}^Q(x_{qs} - \\sigma(\\alpha_s - \\delta_q))$$\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial \\delta_q} = - \\sum_{s=1}^S(x_{qs} - \\sigma(\\alpha_s - \\delta_q))$$\n",
    "\n",
    "But since we are going to use TensorFlow, it will calculate the derivatives automatically, so these are just for the information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import tensorflow as tf\n",
    "np.random.seed(1239)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First we generate the test data\n",
    "\n",
    "#The synthetic question:\n",
    "synthetic_questions = np.arange(-1.9, 3.1, 1)\n",
    "synthetic_students = np.arange(0,2,0.1)\n",
    "synthetic_logits = synthetic_students.reshape(-1,1) - synthetic_questions.reshape(1,-1)\n",
    "synthetic_probs = sigmoid(synthetic_logits)\n",
    "synthetic_data = (synthetic_probs > np.random.rand(synthetic_probs.shape[0],synthetic_probs.shape[1])).astype('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0., 1.],\n",
       "       [1., 1., 0., 0., 0.],\n",
       "       [0., 1., 1., 0., 0.],\n",
       "       [1., 1., 1., 0., 0.],\n",
       "       [1., 0., 1., 1., 0.],\n",
       "       [1., 1., 1., 1., 1.],\n",
       "       [1., 1., 0., 0., 0.],\n",
       "       [1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 0.],\n",
       "       [1., 1., 0., 0., 0.],\n",
       "       [1., 1., 1., 0., 1.],\n",
       "       [1., 1., 1., 0., 0.],\n",
       "       [1., 1., 1., 0., 1.],\n",
       "       [1., 1., 0., 0., 0.],\n",
       "       [1., 1., 1., 1., 0.],\n",
       "       [1., 1., 0., 0., 0.],\n",
       "       [1., 1., 1., 1., 1.],\n",
       "       [1., 1., 0., 1., 0.],\n",
       "       [1., 1., 1., 1., 0.],\n",
       "       [0., 1., 1., 1., 0.]])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "synthetic_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_shape = synthetic_data.shape\n",
    "learning_rate = 0.1\n",
    "tf.reset_default_graph()\n",
    "X = tf.placeholder(dtype='float' ,shape=data_shape, name=\"X\")\n",
    "alpha = tf.Variable(initial_value=np.zeros((data_shape[0],1)), name=\"alpha\", dtype='float')\n",
    "delta = tf.Variable(initial_value=np.zeros((1,data_shape[1])), name=\"delta\", dtype='float')\n",
    "log_likelihood = tf.reduce_sum(X * tf.log(tf.sigmoid(alpha-delta)) + (1-X) * tf.log(1-tf.sigmoid(alpha-delta)))\n",
    "cost = -log_likelihood\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "training_op = optimizer.minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Cost = 69.31474\n",
      "Epoch 1000 Cost = 34.937378\n",
      "Epoch 2000 Cost = 34.922993\n",
      "Epoch 3000 Cost = 34.918358\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "n_epochs = 4000\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        if epoch % 1000 == 0:\n",
    "            print(\"Epoch\", epoch, \"Cost =\", cost.eval(feed_dict={X: synthetic_data}))\n",
    "        sess.run(training_op, feed_dict={X: synthetic_data})\n",
    "    \n",
    "    best_alpha = alpha.eval()\n",
    "    best_delta = delta.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2.2087283 ],\n",
       "       [-2.2087283 ],\n",
       "       [-2.208728  ],\n",
       "       [-0.76759684],\n",
       "       [-0.7675968 ],\n",
       "       [ 7.546268  ],\n",
       "       [-2.2087283 ],\n",
       "       [ 7.546268  ],\n",
       "       [ 0.78600675],\n",
       "       [-2.2087283 ],\n",
       "       [ 0.7860067 ],\n",
       "       [-0.76759684],\n",
       "       [ 0.7860067 ],\n",
       "       [-2.2087283 ],\n",
       "       [ 0.78600675],\n",
       "       [-2.2087283 ],\n",
       "       [ 7.546268  ],\n",
       "       [-0.76759684],\n",
       "       [ 0.78600675],\n",
       "       [-0.7675968 ]], dtype=float32)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-3.4253976 , -3.4253976 , -1.4269196 , -0.10277782,  1.1109241 ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_delta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It got the questions in the right order, and the students are also roughly in the right order, but are affected by chance.\n",
    "\n",
    "One of the improvements of this model would be to add priors for $\\alpha$ and $\\delta$, which will cause regularization and the smoothing of both student ability scores and the question difficulty score."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
