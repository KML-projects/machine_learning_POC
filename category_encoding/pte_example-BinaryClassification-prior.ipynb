{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulation studies using Hastie's data\n",
    "\n",
    "The goal of this simulation study is to see how the performance of the Bayesian-encoded model varies for different values of the hyperparameters. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys,os, pathlib\n",
    "current = pathlib.Path(os.getcwd())\n",
    "base = current.parent.parent\n",
    "catenc = base.joinpath('categorical-encoding')\n",
    "sys.path.append(str(catenc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary classification problem\n",
    "\n",
    "For Binary classifier we will work with the example 10.2 of T. Hastie, R. Tibshirani and J. Friedman, \"Elements of Statistical Learning Ed. 2\", Springer, 2009."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_hastie_10_2\n",
    "X_h, y_h = make_hastie_10_2(random_state=2834)\n",
    "X_h = X_h.astype('float16')\n",
    "y_h[y_h==-1]=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now convert the last column to the categorical\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "disczr1 = KBinsDiscretizer(n_bins=20, encode='ordinal', strategy='uniform')\n",
    "cat_column1 = disczr1.fit_transform(X_h[:,-1].reshape(-1, 1)) * 193 % 20 #We want to break the monotonicity\n",
    "disczr2 = KBinsDiscretizer(n_bins=15, encode='ordinal', strategy='uniform')\n",
    "cat_column2 = disczr2.fit_transform(X_h[:,-2].reshape(-1, 1)) * 173 % 20 #We want to break the monotonicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>col_0</th>\n",
       "      <th>col_1</th>\n",
       "      <th>col_2</th>\n",
       "      <th>col_3</th>\n",
       "      <th>col_4</th>\n",
       "      <th>col_5</th>\n",
       "      <th>col_6</th>\n",
       "      <th>col_7</th>\n",
       "      <th>cat1</th>\n",
       "      <th>cat2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.373047</td>\n",
       "      <td>-2.091797</td>\n",
       "      <td>1.708984</td>\n",
       "      <td>-0.275146</td>\n",
       "      <td>-0.398926</td>\n",
       "      <td>1.024414</td>\n",
       "      <td>-0.765137</td>\n",
       "      <td>-0.189331</td>\n",
       "      <td>4.0</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.469238</td>\n",
       "      <td>1.482422</td>\n",
       "      <td>0.573730</td>\n",
       "      <td>1.517578</td>\n",
       "      <td>-0.036804</td>\n",
       "      <td>-0.188110</td>\n",
       "      <td>-0.654785</td>\n",
       "      <td>1.072266</td>\n",
       "      <td>17.0</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.405518</td>\n",
       "      <td>0.231201</td>\n",
       "      <td>-1.037109</td>\n",
       "      <td>-0.901855</td>\n",
       "      <td>-2.525391</td>\n",
       "      <td>0.429199</td>\n",
       "      <td>-1.176758</td>\n",
       "      <td>-0.426025</td>\n",
       "      <td>17.0</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      col_0     col_1     col_2     col_3     col_4     col_5     col_6  \\\n",
       "0 -1.373047 -2.091797  1.708984 -0.275146 -0.398926  1.024414 -0.765137   \n",
       "1  0.469238  1.482422  0.573730  1.517578 -0.036804 -0.188110 -0.654785   \n",
       "2 -0.405518  0.231201 -1.037109 -0.901855 -2.525391  0.429199 -1.176758   \n",
       "\n",
       "      col_7  cat1  cat2  \n",
       "0 -0.189331   4.0  11.0  \n",
       "1  1.072266  17.0  18.0  \n",
       "2 -0.426025  17.0  11.0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictors = pd.DataFrame(X_h[:, 0:-2], columns=[f'col_{i}' for i in range(8)])\n",
    "predictors['cat1'] = cat_column1\n",
    "predictors['cat2'] = cat_column2\n",
    "#predictors['cat1_orig'] = cat_column1\n",
    "#predictors['cat2_orig'] = cat_column2\n",
    "predictors.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(predictors.values, y_h, test_size=0.2, random_state=2834)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy:  1.0\n",
      "Test accuracy:  0.84875\n",
      "AUC:  0.9255\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "model = RandomForestClassifier(n_estimators=400, max_depth=40, random_state=2834, n_jobs=-1) \n",
    "model.fit(X_train, y_train)\n",
    "preds = model.predict_proba(X_test)[:,1]\n",
    "\n",
    "print('Train accuracy: ', accuracy_score(y_train, model.predict(X_train)))\n",
    "print('Test accuracy: ', accuracy_score(y_test, preds.round()))\n",
    "print('AUC: ', roc_auc_score(y_test, preds).round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameter tuning: optimizing for AUC\n",
    "estimators: 400\n",
    "max depth:\n",
    "* 15 | 0.9334 \n",
    "* 17 | 0.9384\n",
    "* 19 | 0.9398\n",
    "* 21 | 0.9415\n",
    "* 25 | 0.9449\n",
    "* 30 | 0.947\n",
    "* 10 | 0.9476"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-validation\n",
    "\n",
    "We really should use cross-validation to avoid overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-validation of the target encoding model\n",
    "\n",
    "First we will train a model using target encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameter (CV score=0.898):\n",
      "{'loo__sigma': 0.01, 'rf__max_depth': 20, 'rf__max_features': 1, 'rf__min_samples_leaf': 2}\n",
      "Test accuracy:  0.89875\n",
      "Wall time: 52.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "from category_encoders.leave_one_out import LeaveOneOutEncoder\n",
    "\n",
    "loo = LeaveOneOutEncoder(cols=['cat1', 'cat2'], sigma=0.05, random_state=2834)\n",
    "rf = RandomForestClassifier(n_estimators=400, max_depth=30, max_features=1, min_samples_leaf=1,\n",
    "                            random_state=2834, n_jobs=-1) \n",
    "pipe = Pipeline(steps=[('loo',loo), ('rf',rf)])\n",
    "\n",
    "param_grid = {\n",
    "    'loo__sigma': [0.01, 0.05],\n",
    "    'rf__max_depth': [15,20],\n",
    "    'rf__max_features' : [1,2],\n",
    "    'rf__min_samples_leaf': [2,3]\n",
    "}\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(predictors.values, y_h, test_size=0.2, random_state=2834)\n",
    "X_train = pd.DataFrame(X_train, columns=predictors.columns)\n",
    "X_test = pd.DataFrame(X_test, columns=predictors.columns)\n",
    "search = GridSearchCV(pipe, param_grid, cv=5, n_jobs=-1,)\n",
    "search.fit(X_train, y_train)\n",
    "print(\"Best parameter (CV score=%0.3f):\" % search.best_score_)\n",
    "print(search.best_params_)\n",
    "test_predict = search.best_estimator_.predict(X_test)\n",
    "print('Test accuracy: ', accuracy_score(y_test, test_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-validation of the probabilistic encoder\n",
    "\n",
    "First we create a class that makes it easier for us to run sklearn cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from category_encoders.pte_utils import EncoderWrapper\n",
    "from category_encoders.posterior_imputation_bc import PosteriorImputationEncoderBC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameter (CV score=0.880):\n",
      "{'classifier__max_depth': 40, 'classifier__max_features': 1, 'classifier__min_samples_leaf': 4, 'encoder__n_draws': 4, 'encoder__prior_samples_ratio': 0.0001}\n",
      "Test accuracy:  0.895\n",
      "Wall time: 5min 53s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "pte = PosteriorImputationEncoderBC(cols=['cat1', 'cat2'], random_state=2834)\n",
    "model = RandomForestClassifier(n_estimators=400, random_state=2834, n_jobs=-1) \n",
    "wrapper_model = EncoderWrapper(pte, model)\n",
    "\n",
    "param_grid = {\n",
    "    'encoder__n_draws': [3,4],\n",
    "    'encoder__prior_samples_ratio': [1E-4, 1E-3],\n",
    "    'classifier__max_depth': [30,40],\n",
    "    'classifier__max_features' : [1,2],\n",
    "    'classifier__min_samples_leaf': [3,4]\n",
    "}\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(predictors.values, y_h, test_size=0.2, random_state=2834)\n",
    "X_train = pd.DataFrame(X_train, columns=predictors.columns)\n",
    "X_test = pd.DataFrame(X_test, columns=predictors.columns)\n",
    "\n",
    "search = GridSearchCV(wrapper_model, param_grid, cv=5, n_jobs=-1)\n",
    "search.fit(X_train, y_train)\n",
    "print(\"Best parameter (CV score=%0.3f):\" % search.best_score_)\n",
    "print(search.best_params_)\n",
    "\n",
    "test_predict = search.best_estimator_.predict(X_test)\n",
    "print('Test accuracy: ', accuracy_score(y_test, test_predict))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([11.76919317, 15.67619405, 15.55459809, 19.69440112, 16.06360021,\n",
       "        11.35519876, 18.79779987, 14.17840004, 23.76259956, 21.74739819,\n",
       "        28.75391932, 32.50232091, 19.90979691, 24.02899761, 27.01699944,\n",
       "        28.68699951, 19.42459788, 11.59499745, 17.6623991 , 15.9279995 ,\n",
       "        13.61619925, 14.35919867, 15.8619998 , 19.30059986, 19.9371984 ,\n",
       "        23.92839899, 28.17279968, 30.18139944, 25.54079967, 21.95979862,\n",
       "        29.01060014, 22.01720052]),\n",
       " 'std_fit_time': array([3.74980673, 1.03061134, 2.07141987, 0.42447696, 4.27303944,\n",
       "        3.90891133, 0.37695909, 3.32639901, 0.48382333, 3.24767522,\n",
       "        3.88100039, 0.39722286, 2.64603977, 0.73065212, 0.93557649,\n",
       "        0.28336398, 4.75345755, 0.89602601, 0.2455634 , 2.37307076,\n",
       "        1.12008744, 1.72549498, 1.75905643, 0.28357636, 1.91257604,\n",
       "        0.54081095, 0.58686841, 1.1041225 , 3.69907088, 1.77019443,\n",
       "        0.84929072, 5.01405428]),\n",
       " 'mean_score_time': array([5.77540083, 2.40260139, 4.7699996 , 1.29859996, 3.66700068,\n",
       "        7.36460061, 1.57340031, 8.94479957, 2.53000035, 6.2632091 ,\n",
       "        6.52520118, 3.67879863, 8.35860157, 4.4440011 , 8.08899903,\n",
       "        3.02160025, 4.4262002 , 7.52703075, 4.42340083, 5.50660005,\n",
       "        4.37120142, 3.3222012 , 7.09659996, 3.47919869, 7.8851995 ,\n",
       "        5.0370019 , 8.64500151, 4.24040008, 4.93560014, 8.09720168,\n",
       "        5.28360057, 2.01499939]),\n",
       " 'std_score_time': array([3.17030979, 0.83223236, 3.27328559, 0.08248493, 3.01613228,\n",
       "        2.54173998, 0.35771845, 3.70657589, 0.37210976, 4.42595619,\n",
       "        3.36566491, 0.51093069, 2.04514279, 0.44418085, 1.99688873,\n",
       "        0.98524367, 1.5875208 , 0.4983878 , 1.16837097, 1.2753293 ,\n",
       "        0.38782318, 1.41652524, 0.91155937, 0.41372848, 1.40276364,\n",
       "        0.10852805, 2.16468319, 0.78217802, 2.70046533, 1.09688288,\n",
       "        0.18874575, 1.67378428]),\n",
       " 'param_classifier__max_depth': masked_array(data=[30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30,\n",
       "                    30, 30, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40,\n",
       "                    40, 40, 40, 40],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__max_features': masked_array(data=[1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1,\n",
       "                    1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__min_samples_leaf': masked_array(data=[3, 3, 3, 3, 4, 4, 4, 4, 3, 3, 3, 3, 4, 4, 4, 4, 3, 3,\n",
       "                    3, 3, 4, 4, 4, 4, 3, 3, 3, 3, 4, 4, 4, 4],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_encoder__n_draws': masked_array(data=[3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3,\n",
       "                    4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_encoder__prior_samples_ratio': masked_array(data=[0.0001, 0.001, 0.0001, 0.001, 0.0001, 0.001, 0.0001,\n",
       "                    0.001, 0.0001, 0.001, 0.0001, 0.001, 0.0001, 0.001,\n",
       "                    0.0001, 0.001, 0.0001, 0.001, 0.0001, 0.001, 0.0001,\n",
       "                    0.001, 0.0001, 0.001, 0.0001, 0.001, 0.0001, 0.001,\n",
       "                    0.0001, 0.001, 0.0001, 0.001],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'classifier__max_depth': 30,\n",
       "   'classifier__max_features': 1,\n",
       "   'classifier__min_samples_leaf': 3,\n",
       "   'encoder__n_draws': 3,\n",
       "   'encoder__prior_samples_ratio': 0.0001},\n",
       "  {'classifier__max_depth': 30,\n",
       "   'classifier__max_features': 1,\n",
       "   'classifier__min_samples_leaf': 3,\n",
       "   'encoder__n_draws': 3,\n",
       "   'encoder__prior_samples_ratio': 0.001},\n",
       "  {'classifier__max_depth': 30,\n",
       "   'classifier__max_features': 1,\n",
       "   'classifier__min_samples_leaf': 3,\n",
       "   'encoder__n_draws': 4,\n",
       "   'encoder__prior_samples_ratio': 0.0001},\n",
       "  {'classifier__max_depth': 30,\n",
       "   'classifier__max_features': 1,\n",
       "   'classifier__min_samples_leaf': 3,\n",
       "   'encoder__n_draws': 4,\n",
       "   'encoder__prior_samples_ratio': 0.001},\n",
       "  {'classifier__max_depth': 30,\n",
       "   'classifier__max_features': 1,\n",
       "   'classifier__min_samples_leaf': 4,\n",
       "   'encoder__n_draws': 3,\n",
       "   'encoder__prior_samples_ratio': 0.0001},\n",
       "  {'classifier__max_depth': 30,\n",
       "   'classifier__max_features': 1,\n",
       "   'classifier__min_samples_leaf': 4,\n",
       "   'encoder__n_draws': 3,\n",
       "   'encoder__prior_samples_ratio': 0.001},\n",
       "  {'classifier__max_depth': 30,\n",
       "   'classifier__max_features': 1,\n",
       "   'classifier__min_samples_leaf': 4,\n",
       "   'encoder__n_draws': 4,\n",
       "   'encoder__prior_samples_ratio': 0.0001},\n",
       "  {'classifier__max_depth': 30,\n",
       "   'classifier__max_features': 1,\n",
       "   'classifier__min_samples_leaf': 4,\n",
       "   'encoder__n_draws': 4,\n",
       "   'encoder__prior_samples_ratio': 0.001},\n",
       "  {'classifier__max_depth': 30,\n",
       "   'classifier__max_features': 2,\n",
       "   'classifier__min_samples_leaf': 3,\n",
       "   'encoder__n_draws': 3,\n",
       "   'encoder__prior_samples_ratio': 0.0001},\n",
       "  {'classifier__max_depth': 30,\n",
       "   'classifier__max_features': 2,\n",
       "   'classifier__min_samples_leaf': 3,\n",
       "   'encoder__n_draws': 3,\n",
       "   'encoder__prior_samples_ratio': 0.001},\n",
       "  {'classifier__max_depth': 30,\n",
       "   'classifier__max_features': 2,\n",
       "   'classifier__min_samples_leaf': 3,\n",
       "   'encoder__n_draws': 4,\n",
       "   'encoder__prior_samples_ratio': 0.0001},\n",
       "  {'classifier__max_depth': 30,\n",
       "   'classifier__max_features': 2,\n",
       "   'classifier__min_samples_leaf': 3,\n",
       "   'encoder__n_draws': 4,\n",
       "   'encoder__prior_samples_ratio': 0.001},\n",
       "  {'classifier__max_depth': 30,\n",
       "   'classifier__max_features': 2,\n",
       "   'classifier__min_samples_leaf': 4,\n",
       "   'encoder__n_draws': 3,\n",
       "   'encoder__prior_samples_ratio': 0.0001},\n",
       "  {'classifier__max_depth': 30,\n",
       "   'classifier__max_features': 2,\n",
       "   'classifier__min_samples_leaf': 4,\n",
       "   'encoder__n_draws': 3,\n",
       "   'encoder__prior_samples_ratio': 0.001},\n",
       "  {'classifier__max_depth': 30,\n",
       "   'classifier__max_features': 2,\n",
       "   'classifier__min_samples_leaf': 4,\n",
       "   'encoder__n_draws': 4,\n",
       "   'encoder__prior_samples_ratio': 0.0001},\n",
       "  {'classifier__max_depth': 30,\n",
       "   'classifier__max_features': 2,\n",
       "   'classifier__min_samples_leaf': 4,\n",
       "   'encoder__n_draws': 4,\n",
       "   'encoder__prior_samples_ratio': 0.001},\n",
       "  {'classifier__max_depth': 40,\n",
       "   'classifier__max_features': 1,\n",
       "   'classifier__min_samples_leaf': 3,\n",
       "   'encoder__n_draws': 3,\n",
       "   'encoder__prior_samples_ratio': 0.0001},\n",
       "  {'classifier__max_depth': 40,\n",
       "   'classifier__max_features': 1,\n",
       "   'classifier__min_samples_leaf': 3,\n",
       "   'encoder__n_draws': 3,\n",
       "   'encoder__prior_samples_ratio': 0.001},\n",
       "  {'classifier__max_depth': 40,\n",
       "   'classifier__max_features': 1,\n",
       "   'classifier__min_samples_leaf': 3,\n",
       "   'encoder__n_draws': 4,\n",
       "   'encoder__prior_samples_ratio': 0.0001},\n",
       "  {'classifier__max_depth': 40,\n",
       "   'classifier__max_features': 1,\n",
       "   'classifier__min_samples_leaf': 3,\n",
       "   'encoder__n_draws': 4,\n",
       "   'encoder__prior_samples_ratio': 0.001},\n",
       "  {'classifier__max_depth': 40,\n",
       "   'classifier__max_features': 1,\n",
       "   'classifier__min_samples_leaf': 4,\n",
       "   'encoder__n_draws': 3,\n",
       "   'encoder__prior_samples_ratio': 0.0001},\n",
       "  {'classifier__max_depth': 40,\n",
       "   'classifier__max_features': 1,\n",
       "   'classifier__min_samples_leaf': 4,\n",
       "   'encoder__n_draws': 3,\n",
       "   'encoder__prior_samples_ratio': 0.001},\n",
       "  {'classifier__max_depth': 40,\n",
       "   'classifier__max_features': 1,\n",
       "   'classifier__min_samples_leaf': 4,\n",
       "   'encoder__n_draws': 4,\n",
       "   'encoder__prior_samples_ratio': 0.0001},\n",
       "  {'classifier__max_depth': 40,\n",
       "   'classifier__max_features': 1,\n",
       "   'classifier__min_samples_leaf': 4,\n",
       "   'encoder__n_draws': 4,\n",
       "   'encoder__prior_samples_ratio': 0.001},\n",
       "  {'classifier__max_depth': 40,\n",
       "   'classifier__max_features': 2,\n",
       "   'classifier__min_samples_leaf': 3,\n",
       "   'encoder__n_draws': 3,\n",
       "   'encoder__prior_samples_ratio': 0.0001},\n",
       "  {'classifier__max_depth': 40,\n",
       "   'classifier__max_features': 2,\n",
       "   'classifier__min_samples_leaf': 3,\n",
       "   'encoder__n_draws': 3,\n",
       "   'encoder__prior_samples_ratio': 0.001},\n",
       "  {'classifier__max_depth': 40,\n",
       "   'classifier__max_features': 2,\n",
       "   'classifier__min_samples_leaf': 3,\n",
       "   'encoder__n_draws': 4,\n",
       "   'encoder__prior_samples_ratio': 0.0001},\n",
       "  {'classifier__max_depth': 40,\n",
       "   'classifier__max_features': 2,\n",
       "   'classifier__min_samples_leaf': 3,\n",
       "   'encoder__n_draws': 4,\n",
       "   'encoder__prior_samples_ratio': 0.001},\n",
       "  {'classifier__max_depth': 40,\n",
       "   'classifier__max_features': 2,\n",
       "   'classifier__min_samples_leaf': 4,\n",
       "   'encoder__n_draws': 3,\n",
       "   'encoder__prior_samples_ratio': 0.0001},\n",
       "  {'classifier__max_depth': 40,\n",
       "   'classifier__max_features': 2,\n",
       "   'classifier__min_samples_leaf': 4,\n",
       "   'encoder__n_draws': 3,\n",
       "   'encoder__prior_samples_ratio': 0.001},\n",
       "  {'classifier__max_depth': 40,\n",
       "   'classifier__max_features': 2,\n",
       "   'classifier__min_samples_leaf': 4,\n",
       "   'encoder__n_draws': 4,\n",
       "   'encoder__prior_samples_ratio': 0.0001},\n",
       "  {'classifier__max_depth': 40,\n",
       "   'classifier__max_features': 2,\n",
       "   'classifier__min_samples_leaf': 4,\n",
       "   'encoder__n_draws': 4,\n",
       "   'encoder__prior_samples_ratio': 0.001}],\n",
       " 'split0_test_score': array([0.79375   , 0.78697917, 0.821875  , 0.82291667, 0.82760417,\n",
       "        0.82447917, 0.78802083, 0.81510417, 0.82291667, 0.840625  ,\n",
       "        0.84114583, 0.84322917, 0.8328125 , 0.840625  , 0.82135417,\n",
       "        0.83958333, 0.8015625 , 0.76927083, 0.81197917, 0.828125  ,\n",
       "        0.815625  , 0.81927083, 0.825     , 0.81979167, 0.8390625 ,\n",
       "        0.84166667, 0.83541667, 0.83854167, 0.8390625 , 0.8375    ,\n",
       "        0.8453125 , 0.83072917]),\n",
       " 'split1_test_score': array([0.90104167, 0.89635417, 0.90416667, 0.89947917, 0.8921875 ,\n",
       "        0.9015625 , 0.89947917, 0.8921875 , 0.890625  , 0.88802083,\n",
       "        0.89010417, 0.88854167, 0.8890625 , 0.88541667, 0.88854167,\n",
       "        0.88854167, 0.8984375 , 0.89635417, 0.89895833, 0.896875  ,\n",
       "        0.89739583, 0.89947917, 0.903125  , 0.9015625 , 0.89010417,\n",
       "        0.88177083, 0.89427083, 0.88854167, 0.8890625 , 0.884375  ,\n",
       "        0.88802083, 0.88385417]),\n",
       " 'split2_test_score': array([0.89114583, 0.88020833, 0.88333333, 0.88177083, 0.88697917,\n",
       "        0.88697917, 0.88125   , 0.88645833, 0.87395833, 0.8703125 ,\n",
       "        0.8734375 , 0.87135417, 0.88020833, 0.87760417, 0.87395833,\n",
       "        0.86875   , 0.88697917, 0.88489583, 0.8828125 , 0.88229167,\n",
       "        0.89114583, 0.8828125 , 0.88697917, 0.88229167, 0.87552083,\n",
       "        0.871875  , 0.8765625 , 0.871875  , 0.86875   , 0.86979167,\n",
       "        0.8734375 , 0.86822917]),\n",
       " 'split3_test_score': array([0.89010417, 0.89270833, 0.89010417, 0.8921875 , 0.890625  ,\n",
       "        0.8859375 , 0.88854167, 0.88697917, 0.87864583, 0.87552083,\n",
       "        0.8828125 , 0.87916667, 0.88125   , 0.87864583, 0.87864583,\n",
       "        0.878125  , 0.89010417, 0.88854167, 0.890625  , 0.88958333,\n",
       "        0.89010417, 0.890625  , 0.8921875 , 0.89322917, 0.875     ,\n",
       "        0.878125  , 0.88072917, 0.87916667, 0.878125  , 0.87916667,\n",
       "        0.87864583, 0.871875  ]),\n",
       " 'split4_test_score': array([0.89010417, 0.890625  , 0.89270833, 0.890625  , 0.88489583,\n",
       "        0.88854167, 0.89739583, 0.8921875 , 0.88697917, 0.884375  ,\n",
       "        0.88802083, 0.88229167, 0.884375  , 0.88489583, 0.88541667,\n",
       "        0.8828125 , 0.89166667, 0.88958333, 0.89166667, 0.88177083,\n",
       "        0.89375   , 0.89322917, 0.89322917, 0.8890625 , 0.88958333,\n",
       "        0.88645833, 0.88177083, 0.8828125 , 0.88177083, 0.87760417,\n",
       "        0.88697917, 0.88020833]),\n",
       " 'mean_test_score': array([0.87322917, 0.869375  , 0.8784375 , 0.87739583, 0.87645833,\n",
       "        0.8775    , 0.8709375 , 0.87458333, 0.870625  , 0.87177083,\n",
       "        0.87510417, 0.87291667, 0.87354167, 0.8734375 , 0.86958333,\n",
       "        0.8715625 , 0.87375   , 0.86572917, 0.87520833, 0.87572917,\n",
       "        0.87760417, 0.87708333, 0.88010417, 0.8771875 , 0.87385417,\n",
       "        0.87197917, 0.87375   , 0.8721875 , 0.87135417, 0.8696875 ,\n",
       "        0.87447917, 0.86697917]),\n",
       " 'std_test_score': array([0.0399525 , 0.04154592, 0.02906903, 0.02781582, 0.02456302,\n",
       "        0.02710416, 0.04196689, 0.02984043, 0.0245723 , 0.01678797,\n",
       "        0.01792877, 0.01584224, 0.02059604, 0.01670893, 0.02464725,\n",
       "        0.01725397, 0.0362877 , 0.04837126, 0.03202613, 0.0244328 ,\n",
       "        0.03109161, 0.02939642, 0.0280427 , 0.02936946, 0.01857967,\n",
       "        0.01588806, 0.02005743, 0.0176703 , 0.01742108, 0.01675886,\n",
       "        0.01554842, 0.01897035]),\n",
       " 'rank_test_score': array([19, 30,  2,  5,  8,  4, 26, 12, 27, 23, 11, 20, 17, 18, 29, 24, 15,\n",
       "        32, 10,  9,  3,  7,  1,  6, 14, 22, 15, 21, 25, 28, 13, 31])}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
